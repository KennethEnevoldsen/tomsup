{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to basic agents and games\n",
    "This tutorial gives a brief introduction to the basic agents and payoff matreces implemented in the tomsup package. It describes the decision making function and its parameters on an overall basis. For an introduction to the more complex theory of mind (ToM) agent, see *introduction_to_tom.ipynb*. For a more in-depth introduction to individual agent see ```ts.valid_agents()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you are in the github folder change the path - not relevant if tomsup is installed via. pip\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")  # go out of the tutorials folder\n",
    "import tomsup as ts  # and import tomsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "penny_comp = ts.PayoffMatrix(name=\"penny_competitive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Random Bias (RB) Agent\n",
    "The RB agent is the simplest possible agent. It simply selects randomly between options 0 and 1, with a specified bias parameter. The bias parameter must be between 0 and 1, and specifies the probability of the RB agent choosing 1. If nothing else is specified, RB uses a default bias parameter of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the RB agent\n",
    "Sir_RB = ts.RB(bias=0.8)\n",
    "# Have it make its choice in the first round\n",
    "Sir_RB.compete(p_matrix=penny_comp, agent=0, op_choice=None)\n",
    "# Have it make its choice in the second round.\n",
    "Sir_RB.compete(p_matrix=penny_comp, agent=0, op_choice=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tit-for-Tat (TFT) Agent\n",
    "The TFT agent rewards cooperative behavior with cooperation, and punishes deceptive behavior with deception. This is operationalized generally as choosing what the opponent chose in the last round. The TFT has one parameter: the probability of it copying the opponent. As default, this probability is 1, but it can be changed to allow for probabilistic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the RB agent\n",
    "Sir_TFT = ts.TFT(copy_prob=0.8)\n",
    "# Have it make its choice in the first round\n",
    "Sir_TFT.compete(p_matrix=penny_comp, agent=0, op_choice=None)\n",
    "# Have it make its choice in the second round.\n",
    "Sir_TFT.compete(p_matrix=penny_comp, agent=0, op_choice=1)\n",
    "# Have it make its choice in the third round.\n",
    "Sir_TFT.compete(p_matrix=penny_comp, agent=0, op_choice=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Win-Stay Loose-Switch (WSLS) Agent\n",
    "The WSLS agent uses the Win-Stay Loose-Switch strategy. This means that it will select the same option during the round after a round which it won, and switch options after those in which it lost. Winning a round is operationalized generally as getting more points than the average of the payoff matrix, which usually is consistent with game-specific operationalisations. During the first round, the WSLS selects a random option. The WSLS agent has two parameters: it's probability of staying when winning and of switching when losing. As default, both parameters are 1, but they can be changed to allow for probabilistic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the RB agent\n",
    "Sir_WSLS = ts.WSLS(prob_stay=0.9, prob_switch=1)\n",
    "# Have it make its choice in the first round\n",
    "Sir_WSLS.compete(p_matrix=penny_comp, agent=0, op_choice=None)\n",
    "\n",
    "# Have it make its choice in the second round.\n",
    "Sir_WSLS.compete(p_matrix=penny_comp, agent=0, op_choice=1)\n",
    "# Have it make its choice in the third round.\n",
    "Sir_WSLS.compete(p_matrix=penny_comp, agent=0, op_choice=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-Learning (QL) Agent\n",
    "The QL agent implements a simple reinforcement learning algorithm to update beliefs of the value of the two choice options based on experience. It only updates the believed value of the chosen option each turn, depending on the reward it gets. It has two important model parameters. One is the learning rate, which must be between 0 and 1, and determines the size of the belief updates (where 0 is no update and 1 is complete update to the last experienced value). The other is the behavioral temperature, which must be above 0, and which determines the degree to which the behavior of the QL agent is random. Lower temperature values result in deterministic behavior where the agent simply chooses the option with the highest believed value, while higher temperatures giver more probabilistic behavior. During the first round, the QL agent makes a choice based on its starting beliefs, which can also be specified as a model parameter. As default, the QL uses a medium learning rate of 0.5, a very low temperature of 0.001, and agnostic starting beliefs of 0.5 for both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the RB agent\n",
    "Sir_QL = ts.QL(learning_rate=0.7, b_temp=0.3, expec_val=[0.9, 0.5])\n",
    "# Have it make its choice in the first round\n",
    "Sir_QL.compete(p_matrix=penny_comp, agent=0, op_choice=None)\n",
    "# Have it make its choice in the second round.\n",
    "Sir_QL.compete(p_matrix=penny_comp, agent=0, op_choice=1)\n",
    "# Have it make its choice in the third round.\n",
    "Sir_QL.compete(p_matrix=penny_comp, agent=0, op_choice=1)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "21ce6157c268f5f494a0d62668b36ea0f8e183b12d11670489d405f1b6a7fa9a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
